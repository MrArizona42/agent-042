seed: 42

model:
  name_or_path: "mistralai/Mistral-7B-v0.1"
  dtype: "bfloat16"  # use new 'dtype' key; options: "float16", "bfloat16", "float32"
  device_map: "auto"
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  gradient_checkpointing: true
  use_local: true
  local_path: "${paths.project_root}/assets/models/mistral-7b"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

data:
  dataset_name: "scientific_papers"
  dataset_config: "arxiv"
  train_split: "train[:0.5%]"  # keep tiny for memory
  val_split: "validation[:0.5%]"
  max_seq_length: 1024
  batch_size: 1
  num_workers: 0
  use_local: true
  local_path: "${paths.project_root}/assets/datasets/scientific_papers_arxiv"

training:
  lr: 2e-4
  weight_decay: 0.0

trainer:
  max_epochs: 1
  devices: 1
  accelerator: "gpu"
  precision: "bf16"  # match dtype for stability on Ampere+
  gradient_clip_val: 1.0
  accumulate_grad_batches: 8
  log_every_n_steps: 10
  val_check_interval: 100
