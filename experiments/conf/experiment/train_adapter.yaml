seed: 42

model:
  dtype: "bfloat16"
  device_map: "auto"
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  gradient_checkpointing: true
  local_path: "${paths.project_root}/assets/models/ministral/Ministral-3b-instruct"
  offload_folder: "${paths.project_root}/assets/models/offload"

lora:
  r: 2
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

data:
  max_seq_length: 32
  batch_size: 1
  local_path: "${paths.project_root}/assets/datasets/arxiv-summarization-01"

training:
  lr: 2e-4
  weight_decay: 0.0

trainer:
  max_epochs: 1
  devices: 1
  accelerator: "auto"
  precision: "bf16-mixed"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 16
  log_every_n_steps: 100
  val_check_interval: 100

output:
  save_dir: "${paths.project_root}/assets/newly_trained/${now:%Y-%m-%d}/${now:%H-%M-%S}"
