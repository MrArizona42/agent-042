# Разработка и исследование интеллектуального ассистента для исследователей с использованием генерации на основе поиска и эффективного дообучения моделей.

## Инструкции

* `./infra/README.md` - настройка окружения и инфраструктуры
* `./experiments/README.md` - как проводить эксперименты

## Бизнес описание работы агентской системы

Цель проекта: Создать AI-ассистента на базе агентской системы с RAG для исследователей в областях
ML/DL/AI/LLM, который ускоряет поиск информации, суммаризацию научных материалов и генерацию
кода, повышая продуктивность и воспроизводимость исследований. Система работает локально и сохраняет
конфиденциальность пользовательских данных. Она контекстно осведомлена о текущем проекте и
участниках, учитывает историю запросов и выполняемых задач, а при необходимости дополняет знания
внешним поиском — без раскрытия приватной информации. По сути, это «коллега‑ассистент», который
понимает, где и над чем он работает, и помогает принимать решения быстро и безопасно.

### Целевая аудитория

* Исследователи: быстро извлекают суть статей, находят релевантные цитаты и идеи для
  экспериментов.
* ML-инженеры: получают примеры кода, рефакторинг и помощь с интеграцией моделей в пайплайны.
* Студенты и стажеры: получают объяснения концепций и примеры с минимальным входным порогом.
* Руководитель группы: получает обзор прогресса и агрегированные знания по проекту.

### Ключевые сценарии использования

* Чат с поддержкой поиска по внутренним и внешним источникам (RAG): ответы с указанием источников и
  цитат.
* Суммаризация статей и длинных документов (multi-level: от краткого «TL;DR» до подробной
  структуры).
* Генерация и доработка кода (шаблоны, тесты, советы по оптимизации).
* Поиск по корпоративным/локальным репозиториям, базам знаний и коду.

### Основные ценности

* Экономия времени на обзор литературы и поиск решений.
* Быстрая генерация кода и примеров, релевантных имеющимся базам знаний, проектам и репозиториям.

### Ожидаемые возможности системы:

* Чат-бот. Ответы на вопросы про разные области и аспекты ML / DL / AI / LLM.
* Суммаризация документов / статей как отдельная функция.
* Генерация кода как отдельная функция.
* Поддержка контекста с базой знаний (статьи, документация, кодовые базы и т.д.) на базе RAG
  системы как отдельная функция.

### Возможные расширения функционала, возможностей RAG системы и решение проблемы cutoff-date:

* Агентская система с автоматизированным выбором инструментов: LoRA для суммаризации / генерации
  кода, задействование RAG, web-search.
* Хранение и динамическое обновление информации о пользователе и истории переписки. Поддержка
  агента в состоянии постоянного пребывания в контексте той системы, в которой он работает.

## Метрики успеха ответов в чате

### 1. Качество ответа

#### a) Релевантность и корректность

* **Что измеряет:** Насколько ответ соответствует вопросу; фактологическая корректность.
* **Как измерять:** LLM-as-judge с фиксированным рубрикатором: Релевантность (1–5) и Корректность (
  1–5).
* **Датасеты:** Natural Questions, HotpotQA, а также собственные ML/AI вопросы.
* **Входные данные:** Вопрос + извлечённые фрагменты (retrieved passages) + сгенерированный ответ.
* **Судьи / Judges:** Сильная модель (по типу GPT‑4 / Claude / Llama‑70B или аналог).

#### b) Дополнительные метрики

* ROUGE‑L
* BERTScore

### 2. Качество суммаризации

#### a) Покрытие и достоверность

* **Что измеряет:** Покрывает ли суммаризация все аспекты вопроса; сохранены ли ключевые идеи
  (faithfulness).
* **Как измерять:** LLM-as-judge с двумя вопросами:
    1) «Вводит ли суммаризация неподтверждённые утверждения?»
    2) «Покрывает ли основные аспекты вопроса?»
* **Датасеты:** ArXiv, OpenReview, PubMed.

#### b) Дополнительные метрики

* ROUGE‑L
* BERTScore

### 3. Качество генерации кода

#### a) Исполнимость и корректность

* **Что измеряет:** Генерирует ли модель исполняемый код, проходящий тесты.
* **Как измерять:**
    * `Executable rate = runnable_outputs / total_code_outputs`
    * `Correctness rate = passed_tests / total_tests`
* **Датасет:** HumanEval (https://github.com/google-research/google-research/tree/master/human_eval)

## Метрики успеха RAG-системы

### 4. Обоснованность ответов (RAG Grounding)

#### a) Доля обоснованных утверждений (Grounded Answer Rate)

* **Что измеряет:** Насколько утверждения в ответе опираются на извлечённые документы.
* **Как измерять:** Для каждого атомарного утверждения проверить, поддерживается ли оно хотя бы
  одним извлечённым чанком. `Groundedness = supported_claims / total_claims`.
* **Входные данные:** Ответ + извлечённые чанки.
* **Подход:** LLM‑as‑judge или правила сопоставления цитат/фрагментов.

#### b) Сходство ответа и контекста

* **Что измеряет:** Согласованность ответа с предоставленным контекстом.
* **Как измерять:** BERTScore или косинусное сходство эмбеддингов между ответом и агрегированным
  контекстом.
* **Назначение:** Дополнительный сигнал; не заменяет проверку обоснованности.

### 5. Качество извлечения (Retrieval)

#### a) Recall@k

* **Что измеряет:** Доля запросов, для которых хотя бы один релевантный документ попал в top‑k.
* **Как измерять:** Для каждого запроса проверять наличие хотя бы одного «золотого» документа в
  top‑k.
* **Датасеты:** MS MARCO, BEIR.

#### b) nDCG@k

* **Что измеряет:** Качество ранжирования с учётом релевантности на разных позициях.
* **Как измерять:** Вычисление nDCG на top‑k для набора запросов; сравнение с baseline.

## Метрики успеха агентского сервиса

TBD

## Техническое описание и постановка задачи

Сервис будет иметь 2 основные платформы:

1. Платформа для экспериментирования и обучения моделей и адаптеров
2. Платформа с работающим LLM сервисом. Сам сервис будет строиться в 4 этапа:
    1. Базовая LLM.
    2. Базовая LLM с адаптерами под разные нужды. Фиксированный или rule-based выбор адаптеров
    3. Базовая LLM с адаптерами + RAG система. Фиксированный или rule-based выбор адаптеров и RAG
       пайплайна.
    4. Агентский сервис с динамическим выбором задействуемых инструментов

### Платформа для экспериментов и обучение LoRA

* DVC with Yandex Cloud S3 remote
* MLFlow with Yandex Cloud S3 remote
* Hydra для конфигурирования тренировок
* Lightning AI (Pytorch Lightning) для организации тренировочных пайплайнов

### Базовая LLM.

* Клиент делает запрос
* Запрос попадает в API Gateway (FastAPI)
* FastAPI использует Task Router, который на данный момент имеет одну функцию: chat
* FastAPI использует Prompt Builder, который собирает промпт
    * Промпт фиксированный
    * Собирается Prompt Config
* vLLM Inference Server всегда имеет загруженную базовую LLM
    * получает информацию, какой адаптер подгружать (на данный момент никакой)
    * получает промпт
    * vLLM генерирует ответ, который через FastAPI направляется клиенту

### Базовая LLM с адаптерами под разные нужды. Фиксированный или rule-based выбор адаптеров

* Клиент делает запрос
* Запрос попадает в API Gateway (FastAPI)
* **FastAPI использует Task Router, который определяет задействуемую функцию: chat / summarize /
  generate code**
    * **таска определяется rule-based по кейвордам или выбирается вручную в UI**
    * **под каждую таску существует свой LoRA**
* FastAPI использует Prompt Builder, который собирает промпт
    * **Промпт фиксированный, но разный для каждой функции**
    * Собирается Prompt Config
* vLLM Inference Server всегда имеет загруженную базовую LLM
    * получает информацию, какой адаптер подгружать (или никакой)
    * получает промпт
    * vLLM генерирует ответ, который через FastAPI направляется клиенту

### Базовая LLM с адаптерами + RAG система. Фиксированный или rule-based выбор адаптеров и RAG пайплайна.

* Клиент делает запрос
* Запрос попадает в API Gateway (FastAPI)
* FastAPI использует Task Router, который определяет, что нужно сделать: chat / summarize /
  generate code
    * таска определяется rule-based по кейвордам или выбирается вручную в UI
    * под каждую таску существует свой LoRA
* FastAPI использует Prompt Builder, который собирает промпт
    * **в UI можно выбрать, использовать ли RAG**
    * **Промпт теперь может собираться при помощи RAG**
    * Собирается Prompt Config
* vLLM Inference Server всегда имеет загруженную базовую LLM
    * получает информацию, какой адаптер подгружать (или никакой)
    * получает промпт
    * vLLM генерирует ответ, который через FastAPI направляется клиенту

### Агентский сервис с динамическим выбором задействуемых инструментов

* Клиент делает запрос
* Запрос попадает в API Gateway (FastAPI)
* **Между FastAPI и Task Router / Prompt Builder теперь есть еще один слой абстракции с отдельной
  LLM, которая автоматизирует выбор адаптеров и RAG, а также может задействовать другие
  инструменты.**
    * **Подробности TBD**
* FastAPI использует Task Router, который определяет, что нужно сделать: chat / summarize /
  generate code
    * **в UI по-прежнему можно вручную выбрать, какую задачу нужно выполнять**
    * под каждую таску существует свой LoRA
* FastAPI использует Prompt Builder, который собирает промпт
    * **в UI по-прежнему можно выбрать, использовать ли RAG**
    * Собирается Prompt Config
* vLLM Inference Server всегда имеет загруженную базовую LLM
    * получает информацию, какой адаптер подгружать (или никакой)
    * получает промпт
    * vLLM генерирует ответ, который через FastAPI направляется клиенту

### RAG пайплайны

RAG-система разрабатывается отдельно для каждой ключевой функции агента: чат, суммаризация и
генерация кода. Такое разделение обусловлено тем, что эти функции решают разные когнитивные задачи и
предъявляют различные требования к данным, retrieval-стратегиям и интеграции контекста в LLM.

Для каждой RAG-подсистемы планируется проведение экспериментов по следующим направлениям:

* **Данные**: какие типы источников и знаний необходимо включать в векторную БД для конкретной
  задачи.
* **Chunking стратегии**: способы разбиения документов на чанки и их влияние на качество retrieval и
  генерации.
* **Retrieval стратегии**: сравнение sparse, dense и hybrid подходов.
* **Reranking стратегии**: методы переупорядочивания извлечённых чанков для повышения целевых метрик
  качества.

#### Chat

---

**Data**

Для чат-функции агент ориентирован на исследовательские и объяснительные запросы. Используются
разнородные источники знаний:

* научные статьи (arXiv, конференции NeurIPS, ICLR, ICML);
* технические и концептуальные блоги по ML/DL;
* официальная документация библиотек (PyTorch, HuggingFace);
* дополнительные справочные материалы (глоссарии, обзоры).

Цель — обеспечить широкий, но релевантный контекст для ответов на открытые исследовательские
вопросы.

**Chunking**

Планируются эксперименты со следующими стратегиями:

* фиксированные чанки по числу токенов (baseline);
* структурно-осознанный chunking (разделы статьи, заголовки, абзацы);
* семантический chunking для блогов и объяснительных текстов;
* sliding window для улучшения recall при длинных документах.

Оценивается влияние chunking на полноту retrieval и связность ответов.

**Retrieval**

Будут рассмотрены:

* dense retrieval на основе эмбеддингов;
* sparse retrieval (BM25) для повышения точности по ключевым терминам;
* hybrid retrieval, объединяющий оба подхода;
* multi-query retrieval для обработки сложных и неоднозначных вопросов.

**Reranking**

Для улучшения качества контекста:

* cross-encoder reranking для top-k чанков;
* LLM-based reranking (LLM-as-judge);
* reranking с учётом метаданных (тип источника, год публикации).

#### Summarization

---

**Data**

Для суммаризации основной источник данных — длинные документы, предоставляемые пользователем:

* научные статьи;
* технические отчёты;
* длинные блог-посты и обзоры.

Дополнительно возможен retrieval внешнего контекста (фоновые знания), если это улучшает полноту и
точность суммаризации.

**Chunking**

Ключевой аспект для суммаризации. Планируются эксперименты с:

* section-aware chunking (введение, метод, эксперименты, выводы);
* иерархическим chunking (чанки → под-суммаризации → итоговая суммаризация);
* sliding window для сохранения связности.

**Retrieval**

Retrieval используется в двух режимах:

* внутридокументный retrieval (поиск релевантных частей исходного текста);
* опциональный внешний retrieval для добавления контекста (например, определений или связанных
  работ).

**Reranking**

Реранжирование направлено на:

* выбор наиболее информативных чанков документа;
* баланс между покрытием содержания и избыточностью;
* приоритизацию ключевых разделов (методология, результаты).

#### Code generation

---

**Data**

RAG для генерации кода использует специализированные источники:

* официальную документацию ML/DL библиотек;
* примеры кода и туториалы;
* отобранные GitHub-репозитории;
* собственный курируемый набор шаблонов и сниппетов.

Данные строго фильтруются по актуальности и версии библиотек.

**Chunking**

Используются code-aware стратегии:

* разбиение по функциям, классам и API-блокам;
* отделение комментариев и описаний от исполняемого кода;
* небольшие чанки для повышения точности retrieval.

**Retrieval**

Эксперименты включают:

* dense retrieval для поиска семантически релевантных примеров;
* hybrid retrieval (ключевые слова + эмбеддинги) для API-точности;
* retrieval с фильтрацией по метаданным (язык, библиотека, версия).

**Reranking**

* cross-encoder для оценки соответствия задаче;
* LLM-based reranking с фокусом на исполнимость и корректность;
* совместное использование RAG и LoRA-дообученной модели для генерации кода.

## Данные и датасеты

### 1. Данные для обучения и адаптации моделей (Supervised / Instruction Tuning)

#### 1.1. Суммаризация научных документов

**Основной датасет:**

* ccdv/arxiv-summarization

**Формат:**

* article → abstract

**Назначение:**

* обучение и дообучение LoRA-адаптеров для задач суммаризации;
* оценка способности модели извлекать ключевые идеи без искажений (faithfulness).

**Особенности и требования:**

* длинный входной контекст (до десятков тысяч токенов);
* необходимость section-aware preprocessing;
* удаление формул и ссылок либо их нормализация;
* сохранение структуры (Introduction / Method / Results / Conclusion).

**Использование в проекте:**

* fine-tuning LoRA для summarization task;
* offline evaluation (ROUGE-L, BERTScore);
* онлайн-оценка через LLM-as-judge.

#### 1.2. Генерация и модификация кода

**Основной датасет:**

* nvidia/OpenCodeInstruct

**Назначение:**

* обучение LoRA-адаптеров под code generation;
* улучшение соответствия инструкциям;
* снижение синтаксических и API-ошибок.

**Особенности и требования:**

* фильтрация по языку (Python);
* фильтрация по домену (ML/DL);
* контроль версий библиотек (PyTorch, Transformers);
* дедупликация близких примеров.

**Использование в проекте:**

* LoRA fine-tuning под code task;
* генерация unit-тестируемого кода;
* последующая проверка через HumanEval.

### 2. Данные для оценки (Evaluation Datasets)

#### 2.1. Оценка генерации кода

**Датасет:**

* openai/openai_humaneval

**Использование:**

* автоматическая оценка исполнимости;
* измерение correctness rate;
* regression testing при изменениях модели или RAG.

**Метрики:**

* Executable rate;
* Test pass rate.

#### 2.2. Вопрос–ответ и retrieval

**Внешние датасеты:**

* Natural Questions
* HotpotQA
* MS MARCO
* BEIR

**Использование:**

* offline оценка retrieval (Recall@k, nDCG@k);
* проверка groundedness RAG-ответов;
* сравнение dense / sparse / hybrid retrieval.

### 3. Данные для RAG-системы (Knowledge Corpora)

RAG использует недетерминированные, обновляемые источники, которые не входят в supervised-датасеты.

**Типы источников:**

* статьи arXiv и конференций (NeurIPS, ICML, ICLR);
* официальная документация (PyTorch, HuggingFace);
* отобранные технические блоги;
* кодовые базы и репозитории.

**Требования к данным:**

* наличие метаданных (тип источника, дата, версия);
* версионность эмбеддингов;
* воспроизводимость индексации (DVC);
* возможность повторной сборки vector DB.

### 4. Управление данными и воспроизводимость

**DVC используется для:**

* хранения датасетов;
* фиксации preprocessing шагов;
* воспроизводимости экспериментов.

**MLflow:**

* хранение метрик;
* логирование параметров обучения;
* сравнение LoRA-адаптеров.

## Workflow automation and CI/CD

### Branch: experiments

Ветка Experiments используется для экспериментов с обучением LoRA и аналогичных. Пайплайны
экспериментов, которые подтвердили свою успешность, могут быть смержены в Main.

**Pre-commit:**

* black
* isort
* проверка YAML/JSON (Hydra configs)
* trailing whitespace

**CI (опционально, под вопросом):**

* загрузка одного batch из датасета
* dry-run обучения (1–2 шага)

### Branch: develop (inference dev)

Эта ветка используется для разработки всей Inference части сервиса. Push в develop ветку, merge
request из develop в main.

**Pre-commit:**

По сути то же самое, что и в Experiments

* форматирование
* import checks
* базовый linting
* валидация конфигов Hydra

### Branch: main

Эта ветка содержит текущую рабочую версию Inference части сервиса.

**CI (триггерится при открытии merge request):**

Тестирование всех частей сервиса. Суть данных тестов не выполнить полноценную переоценку сервиса по
всем метрикам качества, а понять, что ничего не сломалось.

1. Configuration and data integrity checks
    * валидация всех Hydra-конфигов
    * проверка соответствия task → adapter mappings
    * DVC‑проверки наличия необходимых датасетов и артефактов

2. System-level inference end-to-end tests
    * запуск сервиса (FastAPI + inference backend)
    * smoke‑тесты: chat, summarization, code generation
    * выполнение с включённым и выключенным RAG

3. Retrieval and RAG regression tests
    * фиксированные eval‑запросы и датасеты
    * расчёт Recall@k и nDCG@k для retrieval
    * grounding‑проверки для RAG‑ответов

4. Generation quality sanity checks
    * на небольших фиксированных сплитах
    * ROUGE‑L и BERTScore (summarization)
    * executable rate и correctness rate на небольшой части датасета HumanEval (code)

5. Performance sanity checks
    * latency (p50/p95)
    * memory usage
    * service startup time

**CD (триггерится при одобрении merge request):**

* сборка артефакта inference‑сервиса
* деплой с фиксированными версиями:
    * базовая модель
    * набор LoRA‑адаптеров
    * индекс RAG
* сохранение rollback‑артефактов предыдущего стабильного релиза
* полная и воспроизводимость версии
