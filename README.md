# Обучение LoRA базовой LLM под специфические нужды.

## Постановка задачи

Выбрана тема ВКР “Разработка и исследование интеллектуального ассистента для исследователей с
использованием генерации на основе поиска и эффективного дообучения моделей”. В проекте планируется
реализовать первую часть ВКР - выбор базовой LLM и обучение адаптеров под специфические данные.
Будет выполнен такой план-минимум:

* Поиск готовой LLM, способной вести диалог (например Mistral 7B или что-то подобное).
* Засервить базовую LLM и оформить UI
* Дообучить адаптер в LLM на специфичных данных. Первыми адаптерами хотелось бы добавить адаптер для
  суммаризации текстов и адаптер для задач генерации кода.
* Сравнить качество предсказаний у базовой LLM и LLM с дообученными адаптерами.

Дальнейшие шаги, которые, скорее всего, будут выполняться уже после окончания курса:

* Объединение адаптеров в один chain-of-thought
* Добавление RAG системы
* Добавление в агента возможности взаимодействовать с окружающим миром

## Формат входных и выходных данных

Основная модель, с которой будет происходить взаимодействие, будет принимать на вход текст и
выдавать на выходе текст.

### Метрики

* Оба адаптера будут проверяться метриками оценки генерации.
* Основны: BLEU, CodeBLEU - измеряет пресечение слов / биграмм в исходных и сгенерированных текстах.
* Для суммаризации: BERTScore - измеряет семантическую близость исходных и сгенерированных текстов.
* Для генерации кода: pass@k на бенчмарках (например HumanEval) - измеряет вероятность, что хотя бы
  один из топ-К ответов правильный

## Валидация и тест

Разделяющее правило будет выбираться в зависимости от размера датасета. Правило большого пальца,
которое будет использоваться: иметь не менее 100000 размеченных образцов, 90% из которых будет
использоваться для тренировки (т.е. 90000), и по 5000 на валидацию и тест. В зависимости от наличия
данных и наличия мощностей, эти цифры могут быть увеличены.

## Датасеты

* https://huggingface.co/datasets/ccdv/arxiv-summarization - датасет статей arxiv с разметкой
  “article - abstract”. Уже имеет разбиение train / validation / test примерно 200000 / 6000 / 6000.
  Будет использоваться дефолтное разбиение.
* https://huggingface.co/datasets/nvidia/OpenCodeInstruct - датасет с разметкой “input human
  instruction - code implementation”. Датасет имеет 5млн размеченных промптов. В зависимости от
  ресурсов доступных при обучении, будет выбрано не менее 100000 / 5000 / 5000 семплов.
* https://huggingface.co/datasets/openai/openai_humaneval - датасет с 164 задачами для тестирования
  результатов, оценка по Global leaderboard

## Моделирование

### Бейзлайн

В качестве бейзлайна будут использоваться базовые Mistral 7b,Llama 7b или Llama 3 8b.

### Основная модель

Mistral 7b - LLM на базе архитектуры Трансформер с некоторыми доработками, в числе которых Group
Attention и Sliding Window Attention. Модель обучена на классической задаче предсказания следующего
токена. Уже в базовом варианте неплохо справляется с задачами чат-ассистентов, суммаризации текстов
и генерации кода.

## Внедрение

UI для взаимодействия с моделью - StreamLit. Serving модели - Hugging Face Text Generation
Inference. На первом этапе все сервисы будут обернуты в Docker Compose.

## Инструкции

### Setup

Настройка окружения для разработки и минимальной Infra

Клонируем репозиторий  
Переходим в корень проекта

```bash
uv sync
```

### Usage

Команды для запуска разных функций проекта (тренировка, валидация, инференс и т.д.)

